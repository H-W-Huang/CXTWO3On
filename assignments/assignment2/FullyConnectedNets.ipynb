{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully-Connected Neural Nets\n",
    "In the previous homework you implemented a fully-connected two-layer neural network on CIFAR-10. The implementation was simple but not very modular since the loss and gradient were computed in a single monolithic function. This is manageable for a simple two-layer network, but would become impractical as we move to bigger models. Ideally we want to build networks using a more modular design so that we can implement different layer types in isolation and then snap them together into models with different architectures.\n",
    "\n",
    "In this exercise we will implement fully-connected networks using a more modular approach. For each layer we will implement a `forward` and a `backward` function. The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass, like this:\n",
    "\n",
    "```python\n",
    "def layer_forward(x, w):\n",
    "  \"\"\" Receive inputs x and weights w \"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  out = # the output\n",
    "   \n",
    "  cache = (x, w, z, out) # Values we need to compute gradients\n",
    "   \n",
    "  return out, cache\n",
    "```\n",
    "\n",
    "The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n",
    "\n",
    "```python\n",
    "def layer_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Receive dout (derivative of loss with respect to outputs) and cache,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Unpack cache values\n",
    "  x, w, z, out = cache\n",
    "  \n",
    "  # Use values in cache to compute derivatives\n",
    "  dx = # Derivative of loss with respect to x\n",
    "  dw = # Derivative of loss with respect to w\n",
    "  \n",
    "  return dx, dw\n",
    "```\n",
    "\n",
    "After implementing a bunch of layers this way, we will be able to easily combine them to build classifiers with different architectures.\n",
    "\n",
    "In addition to implementing fully-connected networks of arbitrary depth, we will also explore different update rules for optimization, and introduce Dropout as a regularizer and Batch/Layer Normalization as a tool to more efficiently optimize deep networks.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run the following from the cs231n directory and try again:\n",
      "python setup.py build_ext --inplace\n",
      "You may also need to restart your iPython kernel\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.classifiers.fc_net import *\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from cs231n.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_val: ', (1000L, 3L, 32L, 32L))\n",
      "('X_train: ', (49000L, 3L, 32L, 32L))\n",
      "('X_test: ', (1000L, 3L, 32L, 32L))\n",
      "('y_val: ', (1000L,))\n",
      "('y_train: ', (49000L,))\n",
      "('y_test: ', (1000L,))\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in list(data.items()):\n",
    "  print(('%s: ' % k, v.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine layer: foward\n",
    "Open the file `cs231n/layers.py` and implement the `affine_forward` function.\n",
    "\n",
    "Once you are done you can test your implementaion by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_forward function:\n",
      "difference:  9.76984772881e-10\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_forward function\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    " \n",
    "    \n",
    "## 涉及到的方法的简单介绍\n",
    "## np.linspace() 在指定的间隔内返回均匀间隔的数字。\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out, _ = affine_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# Compare your output with ours. The error should be around e-9 or less.\n",
    "print('Testing affine_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine layer: backward\n",
    "Now implement the `affine_backward` function and test your implementation using numeric gradient checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_backward function:\n",
      "dx error:  1.09081995087e-10\n",
      "dw error:  2.17526355046e-10\n",
      "db error:  7.73697883449e-12\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_backward function\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "# The error should be around e-10 or less\n",
    "print('Testing affine_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU activation: forward\n",
    "Implement the forward pass for the ReLU activation function in the `relu_forward` function and test your implementation using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_forward function:\n",
      "difference:  4.99999979802e-08\n"
     ]
    }
   ],
   "source": [
    "# Test the relu_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be on the order of e-8\n",
    "print('Testing relu_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU activation: backward\n",
    "Now implement the backward pass for the ReLU activation function in the `relu_backward` function and test your implementation using numeric gradient checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_backward function:\n",
      "dx error:  3.27563491363e-12\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# The error should be on the order of e-12\n",
    "print('Testing relu_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1: \n",
    "\n",
    "We've only asked you to implement ReLU, but there are a number of different activation functions that one could use in neural networks, each with its pros and cons. In particular, an issue commonly seen with activation functions is getting zero (or close to zero) gradient flow during backpropagation. Which of the following activation functions have this problem? If you consider these functions in the one dimensional case, what types of input would lead to this behaviour?\n",
    "1. Sigmoid\n",
    "2. ReLU\n",
    "3. Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "1., 2.,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Sandwich\" layers\n",
    "There are some common patterns of layers that are frequently used in neural nets. For example, affine layers are frequently followed by a ReLU nonlinearity. To make these common patterns easy, we define several convenience layers in the file `cs231n/layer_utils.py`.\n",
    "\n",
    "For now take a look at the `affine_relu_forward` and `affine_relu_backward` functions, and run the following to numerically gradient check the backward pass:\n",
    "\n",
    "这些函数仅仅是将两个层（其中一个为激活层）的forward和backward合起来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_relu_forward and affine_relu_backward:\n",
      "dx error:  6.39553504205e-11\n",
      "dw error:  8.16201110576e-11\n",
      "db error:  7.82672402146e-12\n"
     ]
    }
   ],
   "source": [
    "from cs231n.layer_utils import affine_relu_forward, affine_relu_backward\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(2, 3, 4)\n",
    "w = np.random.randn(12, 10)\n",
    "b = np.random.randn(10)\n",
    "dout = np.random.randn(2, 10)\n",
    "\n",
    "out, cache = affine_relu_forward(x, w, b)\n",
    "dx, dw, db = affine_relu_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_relu_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_relu_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_relu_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "# Relative error should be around e-10 or less\n",
    "print('Testing affine_relu_forward and affine_relu_backward:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss layers: Softmax and SVM\n",
    "You implemented these loss functions in the last assignment, so we'll give them to you for free here. You should still make sure you understand how they work by looking at the implementations in `cs231n/layers.py`.\n",
    "\n",
    "You can make sure that the implementations are correct by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing svm_loss:\n",
      "loss:  8.9996027491\n",
      "dx error:  1.40215660067e-09\n",
      "\n",
      "Testing softmax_loss:\n",
      "loss:  2.3025458445\n",
      "dx error:  9.38467316199e-09\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "num_classes, num_inputs = 10, 50\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: svm_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = svm_loss(x, y)\n",
    "\n",
    "# Test svm_loss function. Loss should be around 9 and dx error should be around the order of e-9\n",
    "print('Testing svm_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: softmax_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = softmax_loss(x, y)\n",
    "\n",
    "# Test softmax_loss function. Loss should be close to 2.3 and dx error should be around e-8\n",
    "print('\\nTesting softmax_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-layer network\n",
    "In the previous assignment you implemented a two-layer neural network in a single monolithic class. Now that you have implemented modular versions of the necessary layers, you will reimplement the two layer network using these modular implementations.\n",
    "\n",
    "Open the file `cs231n/classifiers/fc_net.py` and complete the implementation of the `TwoLayerNet` class. This class will serve as a model for the other networks you will implement in this assignment, so read through it to make sure you understand the API. You can run the cell below to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing initialization ... \n",
      "Testing test-time forward pass ... \n",
      "Testing training loss (no regularization)\n",
      "Running numeric gradient check with reg =  0.0\n",
      "W1 relative error: 1.22e-08\n",
      "W2 relative error: 3.50e-10\n",
      "b1 relative error: 8.37e-09\n",
      "b2 relative error: 2.53e-10\n",
      "Running numeric gradient check with reg =  0.7\n",
      "W1 relative error: 2.53e-07\n",
      "W2 relative error: 1.37e-07\n",
      "b1 relative error: 1.56e-08\n",
      "b2 relative error: 9.09e-10\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "N, D, H, C = 3, 5, 50, 7\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=N)\n",
    "\n",
    "std = 1e-3\n",
    "model = TwoLayerNet(input_dim=D, hidden_dim=H, num_classes=C, weight_scale=std)\n",
    "\n",
    "print('Testing initialization ... ')\n",
    "W1_std = abs(model.params['W1'].std() - std)\n",
    "b1 = model.params['b1']\n",
    "W2_std = abs(model.params['W2'].std() - std)\n",
    "b2 = model.params['b2']\n",
    "assert W1_std < std / 10, 'First layer weights do not seem right'\n",
    "assert np.all(b1 == 0), 'First layer biases do not seem right'\n",
    "assert W2_std < std / 10, 'Second layer weights do not seem right'\n",
    "assert np.all(b2 == 0), 'Second layer biases do not seem right'\n",
    "\n",
    "print('Testing test-time forward pass ... ')\n",
    "model.params['W1'] = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H)\n",
    "model.params['b1'] = np.linspace(-0.1, 0.9, num=H)\n",
    "model.params['W2'] = np.linspace(-0.3, 0.4, num=H*C).reshape(H, C)\n",
    "model.params['b2'] = np.linspace(-0.9, 0.1, num=C)\n",
    "X = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T\n",
    "scores = model.loss(X)\n",
    "correct_scores = np.asarray(\n",
    "  [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],\n",
    "   [12.05769098,  12.74614105,  13.43459113,  14.1230412,   14.81149128, 15.49994135,  16.18839143],\n",
    "   [12.58373087,  13.20054771,  13.81736455,  14.43418138,  15.05099822, 15.66781506,  16.2846319 ]])\n",
    "scores_diff = np.abs(scores - correct_scores).sum()\n",
    "assert scores_diff < 1e-6, 'Problem with test-time forward pass'\n",
    "\n",
    "print('Testing training loss (no regularization)')\n",
    "y = np.asarray([0, 5, 1])\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 3.4702243556\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with training-time loss'\n",
    "\n",
    "model.reg = 1.0\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 26.5948426952\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with regularization loss'\n",
    "\n",
    "# Errors should be around e-7 or less\n",
    "for reg in [0.0, 0.7]:\n",
    "  print('Running numeric gradient check with reg = ', reg)\n",
    "  model.reg = reg\n",
    "  loss, grads = model.loss(X, y)\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False)\n",
    "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solver\n",
    "In the previous assignment, the logic for training models was coupled to the models themselves. Following a more modular design, for this assignment we have split the logic for training models into a separate class.\n",
    "\n",
    "Open the file `cs231n/solver.py` and read through it to familiarize yourself with the API. After doing so, use a `Solver` instance to train a `TwoLayerNet` that achieves at least `50%` accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 4900) loss: 2.300184\n",
      "(Epoch 0 / 10) train acc: 0.148000; val_acc: 0.129000\n",
      "(Iteration 101 / 4900) loss: 1.848419\n",
      "(Iteration 201 / 4900) loss: 1.770007\n",
      "(Iteration 301 / 4900) loss: 1.387828\n",
      "(Iteration 401 / 4900) loss: 1.688136\n",
      "(Epoch 1 / 10) train acc: 0.447000; val_acc: 0.438000\n",
      "(Iteration 501 / 4900) loss: 1.692956\n",
      "(Iteration 601 / 4900) loss: 1.713356\n",
      "(Iteration 701 / 4900) loss: 1.422947\n",
      "(Iteration 801 / 4900) loss: 1.394090\n",
      "(Iteration 901 / 4900) loss: 1.473381\n",
      "(Epoch 2 / 10) train acc: 0.476000; val_acc: 0.468000\n",
      "(Iteration 1001 / 4900) loss: 1.300477\n",
      "(Iteration 1101 / 4900) loss: 1.352766\n",
      "(Iteration 1201 / 4900) loss: 1.638424\n",
      "(Iteration 1301 / 4900) loss: 1.172878\n",
      "(Iteration 1401 / 4900) loss: 1.423802\n",
      "(Epoch 3 / 10) train acc: 0.501000; val_acc: 0.477000\n",
      "(Iteration 1501 / 4900) loss: 1.556559\n",
      "(Iteration 1601 / 4900) loss: 1.270026\n",
      "(Iteration 1701 / 4900) loss: 1.287378\n",
      "(Iteration 1801 / 4900) loss: 1.416552\n",
      "(Iteration 1901 / 4900) loss: 1.633102\n",
      "(Epoch 4 / 10) train acc: 0.527000; val_acc: 0.504000\n",
      "(Iteration 2001 / 4900) loss: 1.471286\n",
      "(Iteration 2101 / 4900) loss: 1.361934\n",
      "(Iteration 2201 / 4900) loss: 1.364089\n",
      "(Iteration 2301 / 4900) loss: 1.370125\n",
      "(Iteration 2401 / 4900) loss: 1.226291\n",
      "(Epoch 5 / 10) train acc: 0.545000; val_acc: 0.480000\n",
      "(Iteration 2501 / 4900) loss: 1.363092\n",
      "(Iteration 2601 / 4900) loss: 1.355512\n",
      "(Iteration 2701 / 4900) loss: 1.225932\n",
      "(Iteration 2801 / 4900) loss: 1.320235\n",
      "(Iteration 2901 / 4900) loss: 1.412107\n",
      "(Epoch 6 / 10) train acc: 0.564000; val_acc: 0.508000\n",
      "(Iteration 3001 / 4900) loss: 1.296213\n",
      "(Iteration 3101 / 4900) loss: 1.100510\n",
      "(Iteration 3201 / 4900) loss: 1.343379\n",
      "(Iteration 3301 / 4900) loss: 1.317622\n",
      "(Iteration 3401 / 4900) loss: 1.204882\n",
      "(Epoch 7 / 10) train acc: 0.581000; val_acc: 0.511000\n",
      "(Iteration 3501 / 4900) loss: 1.169569\n",
      "(Iteration 3601 / 4900) loss: 1.275532\n",
      "(Iteration 3701 / 4900) loss: 1.091458\n",
      "(Iteration 3801 / 4900) loss: 1.143065\n",
      "(Iteration 3901 / 4900) loss: 1.202428\n",
      "(Epoch 8 / 10) train acc: 0.566000; val_acc: 0.512000\n",
      "(Iteration 4001 / 4900) loss: 1.031526\n",
      "(Iteration 4101 / 4900) loss: 1.219053\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet()\n",
    "solver = None\n",
    "\n",
    "##############################################################################\n",
    "# TODO: Use a Solver instance to train a TwoLayerNet that achieves at least  #\n",
    "# 50% accuracy on the validation set.                                        #\n",
    "##############################################################################\n",
    "## 定义solver\n",
    "## 涉及到的数据为 data\n",
    "\n",
    "solver = Solver(model,data, \n",
    "                optim_config={\n",
    "                    'learning_rate': 1e-3,\n",
    "                },\n",
    "                lr_decay=0.95,\n",
    "                num_epochs=10, batch_size=100,\n",
    "                print_every=100\n",
    "               )\n",
    "\n",
    "## 开始训练\n",
    "solver.train()\n",
    "### 这样就可以得到一个训练完的model了\n",
    "## 计算测试集\n",
    "# scores,_ = model.loss(data['X_test']) ValueError: too many values to unpack\n",
    "scores = model.loss(data['X_test']) \n",
    "y_pred = np.argmax(scores, axis=1)\n",
    "acc = np.mean( y_pred == data['y_test'])\n",
    "\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run this cell to visualize training loss and train / val accuracy\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(solver.train_acc_history, '-o', label='train')\n",
    "plt.plot(solver.val_acc_history, '-o', label='val')\n",
    "plt.plot([0.5] * len(solver.val_acc_history), 'k--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer network\n",
    "Next you will implement a fully-connected network with an arbitrary number of hidden layers.\n",
    "\n",
    "Read through the `FullyConnectedNet` class in the file `cs231n/classifiers/fc_net.py`.\n",
    "\n",
    "Implement the initialization, the forward pass, and the backward pass. For the moment don't worry about implementing dropout or batch/layer normalization; we will add those features soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial loss and gradient check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, run the following to check the initial loss and to gradient check the network both with and without regularization. Do the initial losses seem reasonable?\n",
    "\n",
    "For gradient checking, you should expect to see errors around 1e-7 or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "for reg in [0, 3.14]:\n",
    "  print('Running check with reg = ', reg)\n",
    "  model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C,\n",
    "                            reg=reg, weight_scale=5e-2, dtype=np.float64)\n",
    "\n",
    "  loss, grads = model.loss(X, y)\n",
    "  print('Initial loss: ', loss)\n",
    "  \n",
    "  # Most of the errors should be on the order of e-7 or smaller.   \n",
    "  # NOTE: It is fine however to see an error for W2 on the order of e-5\n",
    "  # for the check when reg = 0.0\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another sanity check, make sure you can overfit a small dataset of 50 images. First we will try a three-layer network with 100 units in each hidden layer. In the following cell, tweak the learning rate and initialization scale to overfit and achieve 100% training accuracy within 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(Iteration 1 / 40) loss: 3.127617\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 0 / 20) train acc: 0.100000; val_acc: 0.079000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 1 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 2 / 20) train acc: 0.200000; val_acc: 0.087000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 3 / 20) train acc: 0.200000; val_acc: 0.087000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 4 / 20) train acc: 0.220000; val_acc: 0.101000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 5 / 20) train acc: 0.160000; val_acc: 0.078000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(Iteration 11 / 40) loss: 11.075915\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 6 / 20) train acc: 0.120000; val_acc: 0.079000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 7 / 20) train acc: 0.160000; val_acc: 0.080000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 8 / 20) train acc: 0.120000; val_acc: 0.129000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 9 / 20) train acc: 0.200000; val_acc: 0.086000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 10 / 20) train acc: 0.120000; val_acc: 0.129000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(Iteration 21 / 40) loss: 21.909120\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 11 / 20) train acc: 0.160000; val_acc: 0.122000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 12 / 20) train acc: 0.160000; val_acc: 0.081000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 13 / 20) train acc: 0.200000; val_acc: 0.117000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 14 / 20) train acc: 0.220000; val_acc: 0.100000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 15 / 20) train acc: 0.200000; val_acc: 0.118000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(Iteration 31 / 40) loss: 24.846371\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 16 / 20) train acc: 0.180000; val_acc: 0.135000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 17 / 20) train acc: 0.180000; val_acc: 0.126000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 18 / 20) train acc: 0.180000; val_acc: 0.093000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 19 / 20) train acc: 0.220000; val_acc: 0.100000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 20 / 20) train acc: 0.180000; val_acc: 0.136000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAHwCAYAAADuJ7gwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAIABJREFUeJzt3X20XWldJ/jvz1SU24BeoCJNglqoGERLKxBpnEIHUQkoLbHGJfha3U1PaS/t1tYJVhztVkenSjOKtst2yQhSjrwuDIFG7UhXgS+MAilSEBAzFm9t3UBVORJe5A6kwjN/nH2rbmJyc19yznPuuZ/PWlnn7H32Oed3d+1z6nue59n7qdZaAACYrM/qXQAAwFYkhAEAdCCEAQB0IIQBAHQghAEAdCCEAQB0IIQBY1FV26rqE1X1hZdz23XU8fNV9ZLL/boXea9vqqoPrPD4b1fVT06iFmD6XdG7AGA6VNUnli3+kySfSnJ2WP6B1tpL1/J6rbWzSR5yubfdzFpr/3o121XVXUm+t7X2pvFWBPQkhAFJktba/SFoaM351621/3ax7avqitbafZOojdXz3wU2D92RwKoM3XqvrKqXV9XHk3xvVX1tVf1lVZ2uqg9V1X+qqu3D9ldUVauqq4bl3xse/6Oq+nhV/UVVPWat2w6PP7Oq/p+q+mhV/XpVvbmq/sUq/45vr6p3DzXfVlW7lz32k1V1qqo+VlV/XVVPHdY/uarePqy/u6oOXeI9nl9V9w6v9f3L1v9eVf3McP/zq+oPhzr+vqr+dFj/8iQ7k/zR0EX7Y6uo+66qOlBVJ5L8Q1UdrKpXnlfTf66qX17NPgImQwgD1uLbk7wsyecleWWS+5L8SJIrk1yb5BlJfmCF5393kp9O8vAk/z3J/7bWbavq85O8KsmB4X3fn+RJqym+qr48yf+V5N8m2ZHkvyV5XVVtr6qvGGp/Qmvtc5M8c3jfJPn1JIeG9V+a5NUrvM2jk8xlFKR+MMlvVtXnXmC7A0neN9TxT5P8VJK01r4ryakkz2ytPaS19isr1b3s9Z471Dw/bPutS+9bVZ+d5DlJfnc1+wmYDCEMWIs/b639l9baZ1pri621t7XW3tJau6+19r4kL0zyP67w/Fe31o611s4keWmSa9ax7bOS3NFae+3w2AuS/N0q639ukte11m4bnntzRoHyn2UUKB+U5CuGLr33D39TkpxJ8tiqekRr7eOttbes8B7/X5Kfb62daa29LqOxdV92ge3OZBTUvrC19unW2p+us+4lv9Zau2v473JXkr9I8j8Nj31LkoXW2jtWeA9gwoQwYC3+dvlCVT2uqv6gqj5cVR9L8nMZtU5dzIeX3f9kVh6Mf7Ftdy6vo7XWkty1itqXnvvBZc/9zPDcXa21k0l+PKO/4Z6h2/WfDpv+yySPT3Kyqt5aVd+ywnv83XCiwYVqX+7moZZbq+q9VXVgPXUv2+Zvz3vOLUm+d7j/vRm1jgFTRAgD1qKdt/xbSd6V5EuHrrr/kKTGXMOHMuryS5JUVeXcMLKSU0m+aNlzP2t4rYUkaa39Xmvt2iSPSbItyU3D+pOttecm+fwkv5zk96vqQRv5I1prH2ut/fvW2lVJ9if5iapaakU8fz+vWPdFnnM4yROHbtZnZtSaCEwRIQzYiIcm+WhGg8G/PCuPB7tcXp/kCVX1z6vqiozGpO1Y5XNfleTbquqpw3iqA0k+nuQtVfXlVfUNVfU5SRaHf59Jkqr6vqq6cmiB+mhGgeczG/kjhvq/ZAiRH83ociBLr3l3ki9eTd0Xe/3W2ieTvCbJy5O8ubV2aiP1ApefEAZsxI8nuT6jQPBbGQ3WH6vW2t0ZDTL/lST/b5IvSXI8o7FXl3ruuzOq9zeT3JvRiQTfNoyz+pwkv5TR+LIPJ3lYkv91eOq3JHnPcFbo/5HkOa21T2/wT9md5LYkn0jy5ozGdP3Z8Nj/nuRnhzMhf/QSda/kliRXR1ckTKUaDacA2JyqaltG3XXfsSzEkKSqvjjJO5M8srX2D73rAc6lJQzYdKrqGVU1P3Qd/nRGZxq+tXNZU2UYN/ZjSV4mgMF0csV8YDN6SkbXK7siybuTfHtr7ZLdkVtFVX1eRoP2P5BkX99qgIvRHQkA0IHuSACADoQwAIAONsWYsCuvvLJdddVVvcsAALik22+//e9aa5e8fuGmCGFXXXVVjh071rsMAIBLqqoPXnor3ZEAAF0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHVwxzhevqg8k+XiSs0nua63traqHJ3llkquSfCDJd7bWPjLOOgAAps0kWsK+obV2TWtt77B8Y5JbW2uPTXLrsAwXdOT4Qq69+bY85sY/yLU335Yjxxd6lwQAl0WP7shnJ7lluH9Lkv0damATOHJ8IQcPn8jC6cW0JAunF3Pw8AlBDICZMO4Q1pL8cVXdXlU3DOse2Vr70HD/w0keeaEnVtUNVXWsqo7de++9Yy6TaXTo6Mksnjl7zrrFM2dz6OjJThUBwOUz1jFhSZ7SWluoqs9P8oaq+uvlD7bWWlW1Cz2xtfbCJC9Mkr17915wG2bbqdOLa1oPAJvJWFvCWmsLw+09SV6T5ElJ7q6qRyXJcHvPOGtg89o5P7em9QCwmYwthFXVg6vqoUv3kzw9ybuSvC7J9cNm1yd57bhqYHM7sG935rZvO2fd3PZtObBvd6eKAODyGWd35COTvKaqlt7nZa21/1pVb0vyqqp6XpIPJvnOMdbAJrZ/z64ko7Fhp04vZuf8XA7s233/egDYzKq16R9utXfv3nbs2LHeZQAAXFJV3b7s0lwX5Yr5AAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHYw9hFXVtqo6XlWvH5YfU1Vvqao7q+qVVfXZ464BAGDaTKIl7EeSvGfZ8i8meUFr7UuTfCTJ8yZQAwDAVBlrCKuqRyf51iS/PSxXkqclefWwyS1J9o+zBgCAaTTulrBfTfL8JJ8Zlh+R5HRr7b5h+a4ku8ZcAwDA1BlbCKuqZyW5p7V2+zqff0NVHauqY/fee+9lrg4AoK9xtoRdm+TbquoDSV6RUTfkryWZr6orhm0enWThQk9urb2wtba3tbZ3x44dYywTAGDyxhbCWmsHW2uPbq1dleS5SW5rrX1Pkjcm+Y5hs+uTvHZcNQAATKse1wn7iSQ/VlV3ZjRG7EUdagAA6OqKS2+yca21NyV503D/fUmeNIn3BQCYVq6YDwDQgRAGANCBEAYA0IEQBgDQgRAGANCBEAYA0IEQBgDQwUSuE8b6HDm+kENHT+bU6cXsnJ/LgX27s3+P+c4BYBYIYVPqyPGFHDx8IotnziZJFk4v5uDhE0kiiAHADBDCptShoyfvD2BLFs+czaGjJ4UwLkkrKsD0E8Km1KnTi2taD0u0ogJsDgbmT6md83NrWg9LVmpFBWB6CGFT6sC+3Znbvu2cdXPbt+XAvt2dKmKz0IoKsDkIYVNq/55duem6q7Nrfi6VZNf8XG667mrdSVySVlSAzcGYsCm2f88uoYs1O7Bv9zljwhKtqADTSAiDGbMU3J0dCTDdhDCYQVpRAaafMWEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB24Thgz58jxBRcqBWDqCWHMlCPHF86Zsmfh9GIOHj6RJIIYAFNFdyQz5dDRk+fMmZgki2fO5tDRk50qAoALE8KYKadOL65pPQD0IoQxU3bOz61pPQD0IoQxUw7s25257dvOWTe3fVsO7NvdqSIAuDAD85kpS4PvnR0JwLQTwpg5+/fsEroAmHq6IwEAOhDCAAA6EMIAADowJmzGmLIHADYHIWyGmLIHADYP3ZEzxJQ9ALB5CGEzxJQ9ALB5CGEzxJQ9ALB5CGEzxJQ9ALB5GJg/Q0zZAwCbhxA2Y0zZAwCbg+5IAIAOhDAAgA50RwJAB5Oa4cRMKtNLCAOACZvUDCdmUpluuiMBYMImNcOJmVSmmxAGABM2qRlOzKQy3YQwAJiwSc1wYiaV6SaEAcCETWqGEzOpTDcD8wFgwiY1w4mZVKZbtdZ613BJe/fubceOHetdBgDAJVXV7a21vZfaTnckAEAHQhgAQAfGhAFsgKuRA+slhAGsk6uRAxuhOxJgnVyNHNgIIQxgnVyNHNgIIQxgnVyNHNgIIQxgnVyNHNgIA/MB1snVyIGNEMKAdXN5hlEQ22p/M3B5CGHAurg8A8DGGBMGrIvLMwBsjBAGrIvLMwBsjBAGrIvLMwBsjBAGrIvLMwBsjIH5wLq4PAPAxghhwLq5PAPA+umOBADoQAgDAOhACAMA6EAIAwDoQAgDAOhgbCGsqh5UVW+tqndU1bur6meH9Y+pqrdU1Z1V9cqq+uxx1QAAMK3G2RL2qSRPa619dZJrkjyjqp6c5BeTvKC19qVJPpLkeWOsAQBgKq0phNXIg1ezbRv5xLC4ffjXkjwtyauH9bck2b+WGgAAZsElQ1hV/W5VfW5V/ZMk707y/qr6sdW8eFVtq6o7ktyT5A1J3pvkdGvtvmGTu5K40iMAsOWspiXsq1prH8uoxeqPkzw6yb9YzYu31s621q4ZnvOkJI9bbWFVdUNVHauqY/fee+9qnwYAsCmsJoRtr6orkjw7yZHW2qeTfGYtb9JaO53kjUm+Nsn88HrJKJwtXOQ5L2yt7W2t7d2xY8da3g4AYOqtJoT9dpL/nuRhSf6kqr4wySdWfkpSVTuqan64P5fkm5O8J6Mw9h3DZtcnee066gYA2NQuOYF3a+0FSV6wtFxVf5vR4PpLeVSSW6pqW0Zh71WttddX1V8leUVV/XyS40letK7KAQA2sUuGsKr64SS/21r7WFX9VpI9SQ4muXWl57XW3jlse/7692U0PgwAYMtaTXfkDUMAe3pGZzL+myS/NN6yAABm22pCWBtuvyXJ77TWbl/l8wAAuIjVhKl3VNUfJnlWkj+qqofkgWAGAMA6XHJMWJJ/meSJSe5srX2yqq6MqYYAADZkNWdHnh2C13VVlSR/0lr7o7FXBgAww1YzbdEvJHl+kvcN/w4Ml5cAAGCdVtMd+c+TPGFpvseqenGStyf5qXEWBgAwy1Z7luNDL3IfAIB1WE1L2C8leXtV3Zqkkjw1yU+PsygAgFm3moH5v1dVb0zyz4ZV/6G1dsFJtwEAWJ2LhrCq+qrzVt053D6iqh4xTEsEAMA6rNQS9hsrPNaSfP1lrgUAYMu4aAhrrX3dJAsBANhKVjMwH2DTOXJ8IYeOnsyp04vZOT+XA/t2Z/+eXb3LArifEAbMnCPHF3Lw8IksnjmbJFk4vZiDh08kiSAGTI3VXicMYNM4dPTk/QFsyeKZszl09GSnigD+sUu2hF3gLMkk+WiSv22tfebylwSwMadOL65pPUAPq+mOfFGSa5K8O6OLtX55kncl+byquqG1dusY6wNYs53zc1m4QODaOT/XoRqAC1tNd+TfJHlia+2a1tpXJ3likuNJ9iX55XEWB7AeB/btztz2beesm9u+LQf27e5UEcA/tpqWsMcvvzBra+1EVT2htXZnVY2xNID1WRp87+xIYJqtJoTdWVW/nuQVw/Jzkry3qj4nyX1jqwxgA/bv2SV0AVNtNd2R35/kriQ3Dv9OJbk+owD2jeMrDQBgdq1mAu9PJvnF4d/5PnrZKwIA2AJWc4mKJyf5j0m+aPn2rbUvG2NdAAAzbTVjwn4nyfOT3J7k7CW2BQBgFVYTwj7WWvsvY68EAGALWU0Iu62qbkpyOMmnllYuv2wFAABrs5oQ9pTzbpOkJfn6y18OAMDWsJqzI79uEoUAAGwlFw1hVfVdrbWXV9W/u9DjrbX/NL6ymKQjxxdcWRwAJmyllrCHDbc7JlEIfRw5vpCDh09k8czoxNeF04s5ePhEkghiADBGFw1hrbX/PNz+9OTKYdIOHT15fwBbsnjmbA4dPSmEAcAYreZirVcm+VdJrsq5F2u9YXxlMSmnTi+uaT0AcHms5uzI1yb5yyR/HhdrnTk75+eycIHAtXN+rkM1XIgxewCzaTUh7MGttR8feyV0cWDf7nPGhCXJ3PZtObBvd8eqWGLMHsDs+qxVbPNHVfX0sVdCF/v37MpN112dXfNzqSS75udy03VX+x/8lFhpzB4Am9tqWsJ+MMlPVNUnk3w6SSVprbWHj7UyJmb/nl1C15QyZg8mzxAAJmU1IezKsVcBXJAxezBZhgAwSRftjqyqxw53v+Ii/4AxO7Bvd+a2bztnnTF7MD6GADBJK7WE3ZjkeUl+4wKPmTsSJmDpl7euEZgMQwCYpJUu1vq84dbckdCRMXswOYYAMEmrOTsyVfW4qrquqr576d+4CwOASTMEgElazRXzfyrJ05M8LsnRJPsyunDry8ZbGgBMliEATNJqzo58TpJrkry9tfZ9VfWoJL813rIAoA9DAJiU1XRHLrbWzia5r6oemuTDSb54vGUBAMy21bSEHa+q+SQvTnIsyceSvH2sVQEAzLgVQ1hVVZKfaa2dTvIbVXU0yee21oQwAIANWDGEtdZaVb0+yROH5TsnUhUA5zCVDsye1YwJe2tV7Rl7JQBc0NJUOgunF9PywFQ6R44v9C4N2ICLtoRV1RWttfuSPCXJ/1xV703yD3lgAu8nTKhGgC1tpal0tlJrmNZAZs1K3ZFvTfKEJPsnVAsAF7DeqXRmKbSYWJtZtFIIqyRprb13QrUAcAHrmUpn1kKL1kBm0UohbEdV/djFHmyt/coY6gHgPAf27T4nUCWXnkpn1kKLibWZRSuFsG1JHpKhRQyAPtYzlc6shRYTazOLVgphH2qt/dzEKgHgotY6lc6shZb1tAbCtFvpEhVawAA2qQP7dmdu+7Zz1m3m0LJ/z67cdN3V2TU/l0qya34uN1139absWoUlK7WEfePEqgDgslpPF+a0M7E2s+aiIay19veTLASAy0togem2mivmAwBwmQlhAAAdrDiBN8BWMktXmIeN8FmYDCEMILN3hXlYL5+FydEdCZCVrzAPW4nPwuRoCWNiNG8zzWbtCvOT5LM9W3wWJkdLGBOx1Ly9cHoxLQ80bx85vtC7NEhy8SvJb9YrzE+Kz/bs8VmYHCGMidC8zbSbtSvMT4rP9uzxWZgc3ZFMhOZtlkxr19UsXmF+Eny2Z4/PwuQIYUzErE0mzPpM+1lXrjC/dj7bs8lnYTJ0RzIRmrdJdF3NIp9tWD8tYUyE5m0SXVezyGcb1k8IY2I0b6Prajb5bMP66I4EJkbXFcADtIRNyLSeEQaTpOsK4AFC2ARM+xlhMEm6rgBGdEdOgDPCAIDzaQmbAGeEAXA5GNoyW8bWElZVX1BVb6yqv6qqd1fVjwzrH15Vb6iqvxluHzauGqaFebgA2CjzdM6ecXZH3pfkx1trj0/y5CQ/VFWPT3Jjkltba49NcuuwPNOcEQbARhnaMnvGFsJaax9qrb19uP/xJO9JsivJs5PcMmx2S5L946phWuzfsys3XXd1ds3PpZLsmp/LTdddrQkZgFUztGX2TGRMWFVdlWRPkrckeWRr7UPDQx9O8shJ1NCbM8IA2AgXO549Yz87sqoekuT3k/xoa+1jyx9rrbUk7SLPu6GqjlXVsXvvvXfcZQLAVDO0ZfaMNYRV1faMAthLW2uHh9V3V9WjhscfleSeCz23tfbC1tre1treHTt2jLNMAJh6hrbMnrF1R1ZVJXlRkve01n5l2UOvS3J9kpuH29eOqwYAmCWGtsyWcY4JuzbJ9yU5UVV3DOt+MqPw9aqqel6SDyb5zjHWAAAwlcYWwlprf56kLvLwN47rfQEANgPTFgEAdCCEAQB0IIQBAHQghAEAdCCEAQB0IIQBAHQghAEAdCCEAQB0IIQBAHQghAEAdDDOuSMBYEs4cnwhh46ezKnTi9k5P5cD+3abaJtLEsIAYAOOHF/IwcMnsnjmbJJk4fRiDh4+kSSCGCvSHQkAG3Do6Mn7A9iSxTNnc+joyU4VsVkIYQCwAadOL65pPSwRwgBgA3bOz61pPSwRwgBgAw7s25257dvOWTe3fVsO7NvdqSI2CwPzAWADlgbfOzuStRLCAGCD9u/ZJXSxZrojAQA6EMIAADoQwgAAOhDCAAA6EMIAADoQwgAAOhDCAAA6EMIAADoQwgAAOhDCAAA6EMIAADoQwgAAOjCBN5AkOXJ8IYeOnsyp04vZOT+XA/t2m5AYYIyEMCBHji/k4OETWTxzNkmycHoxBw+fSBJBDGBMdEcCOXT05P0BbMnimbM5dPRkp4oAZp8QBuTU6cU1rQdg44QwIDvn59a0HoCNE8KAHNi3O3Pbt52zbm77thzYt7tTRQCzz8D8dXAWGbNm6fh1XANMjhC2Rs4iY1bt37PLMQxM1FZv1NAduUbOIgOAjVtq1Fg4vZiWBxo1jhxf6F3axGgJWyNnkcHkbfVfyzCLVmrU2Cqfby1ha+QsMpgsv5ZhNmnUEMLWzFlkMFmGAMBs0qghhK3Z/j27ctN1V2fX/Fwqya75udx03dVbpukUJs2vZdgcjhxfyLU335bH3PgHufbm2y7ZWq1Rw5iwdXEWGUzOzvm5LFwgcG2lX8sw7dZz5QCXxhHCYKIMMF+7A/t2n/Plnmy9X8sw7dY7yH6rN2oIYTAhrjG3Pn4tw/QzbGB9hDCYEKdjr99W/7UM086wgfUxMB8mxC9FeMBaB3Ez3QyyXx8tYTAhfinCiK752WPYwPoIYTAhBpjDiK752WTYwNoJYTAhfinCiK55GBHCYIL8UgRd87DEwHwAJsogbhjREgbAROmahxEhDICJ0zUPuiMBALoQwgAAOhDCAAA6EMIAADoQwgAAOhDCAAA6EMIAADoQwgAAOhDCAAA6EMIAADoQwgAAOjB3JAAz68jxBROFM7WEMIgvaphFR44v5ODhE1k8czZJsnB6MQcPn0gSn2+mgu5ItrylL+qF04tpeeCL+sjxhd6lARtw6OjJ+wPYksUzZ3Po6MlOFcG5tISxLrPUcrTSF/Vm/ZuA5NTpxTWth0nTEsaazVrLkS9qmE075+fWtB4mTQhjzWatiX+9X9RHji/k2ptvy2Nu/INce/NtmzaEMnmOnck4sG935rZvO2fd3PZtObBvd6eK4FxCGGs2ay1H6/minrXWQCbHsTM5+/fsyk3XXZ1d83OpJLvm53LTdVcbZsDUMCaMNds5P5eFCwSuzdrEv/SFvJYxbsaRsV6Oncnav2eX/crUEsJYswP7dp9z2ney+Zv41/pFPWutgUyOYwdYMrbuyKp6cVXdU1XvWrbu4VX1hqr6m+H2YeN6f8ZHE78Bv6yfYwdYMs4xYS9J8ozz1t2Y5NbW2mOT3Dosswnt37Mrb77xaXn/zd+aN9/4tC0VwBIDflk/xw6wZGzdka21P62qq85b/ewkTx3u35LkTUl+Ylw1wLisZxwZJI4d4AHVWhvfi49C2Otba185LJ9urc0P9yvJR5aWV7J379527NixsdUJAHC5VNXtrbW9l9qu2yUq2ij9XTQBVtUNVXWsqo7de++9E6wMAGD8Jh3C7q6qRyXJcHvPxTZsrb2wtba3tbZ3x44dEysQAGASJh3CXpfk+uH+9UleO+H3BwCYCuO8RMXLk/xFkt1VdVdVPS/JzUm+uar+Jsk3DcsAAFvOOM+O/K6LPPSN43pPAIDNwtyRAAAdCGEAAB0IYQAAHQhhAAAdjG1gPgDANDhyfGEqpwoTwgCAmXXk+EIOHj6RxTNnkyQLpxdz8PCJJOkexHRHAgAz69DRk/cHsCWLZ87m0NGTnSp6gBAGAMysU6cX17R+koQwAGBm7ZyfW9P6SRLCAICZdWDf7sxt33bOurnt23Jg3+5OFT3AwHwA7jetZ5HBei0dv9N4XAthACSZ7rPIYCP279k1lcew7kgAkkz3WWQwi4QwAJJM91lkMIuEMACSTPdZZDCLhDAAkkz3WWQwiwzMByDJdJ9FBrNICAPgftN6FhnMIt2RAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB1c0bsAAIDVOnJ8IYeOnsyp04vZOT+XA/t2Z/+eXb3LWhchDADYFI4cX8jBwyeyeOZskmTh9GIOHj6RJJsyiOmOBAA2hUNHT94fwJYsnjmbQ0dPdqpoY7Z8S9gsNWsCwCw7dXpxTeun3ZZuCVtq1lw4vZiWB5o1jxxf6F0aAHCenfNza1o/7bZ0CJu1Zk0AmGUH9u3O3PZt56yb274tB/bt7lTRxmzp7shZa9YEgFm2NFxoVoYRbekQtnN+LgsXCFybtVkTAGbd/j27Nm3oOt+W7o6ctWZNAGDz2NItYbPWrAkAbB5bOoQls9WsCQBsHlu6OxIAoBchDACgAyEMAKADIQwAoAMhDACgAyEMAKADIQwAoAMhDACgAyEMAKADIQwAoAMhDACgAyEMAKADIQwAoAMhDACgAyEMAKCDaq31ruFIpKK1AAAHDUlEQVSSqureJB8c89tcmeTvxvwe084+sA8S+yCxDxL7ILEPEvsgWd8++KLW2o5LbbQpQtgkVNWx1tre3nX0ZB/YB4l9kNgHiX2Q2AeJfZCMdx/ojgQA6EAIAwDoQAh7wAt7FzAF7AP7ILEPEvsgsQ8S+yCxD5Ix7gNjwgAAOtASBgDQgRCWpKqeUVUnq+rOqrqxdz09VNUHqupEVd1RVcd61zMJVfXiqrqnqt61bN3Dq+oNVfU3w+3DetY4bhfZBz9TVQvDsXBHVX1LzxrHraq+oKreWFV/VVXvrqofGdZvmWNhhX2wZY6FqnpQVb21qt4x7IOfHdY/pqreMvz/4ZVV9dm9ax2XFfbBS6rq/cuOg2t61zpuVbWtqo5X1euH5bEcB1s+hFXVtiS/keSZSR6f5Luq6vF9q+rmG1pr12yh05FfkuQZ5627McmtrbXHJrl1WJ5lL8k/3gdJ8oLhWLimtfaHE65p0u5L8uOttccneXKSHxq+A7bSsXCxfZBsnWPhU0me1lr76iTXJHlGVT05yS9mtA++NMlHkjyvY43jdrF9kCQHlh0Hd/QrcWJ+JMl7li2P5TjY8iEsyZOS3Nlae19r7dNJXpHk2Z1rYgJaa3+a5O/PW/3sJLcM929Jsn+iRU3YRfbBltJa+1Br7e3D/Y9n9MW7K1voWFhhH2wZbeQTw+L24V9L8rQkrx7Wz/pxcLF9sKVU1aOTfGuS3x6WK2M6DoSw0RfN3y5bvitb7Mtn0JL8cVXdXlU39C6mo0e21j403P9wkkf2LKajH66qdw7dlTPbDXe+qroqyZ4kb8kWPRbO2wfJFjoWhi6oO5Lck+QNSd6b5HRr7b5hk5n//8P5+6C1tnQc/MJwHLygqj6nY4mT8KtJnp/kM8PyIzKm40AIY8lTWmtPyKhb9oeq6ut7F9RbG506vOV+BSb5zSRfklF3xIeS/HLfciajqh6S5PeT/Ghr7WPLH9sqx8IF9sGWOhZaa2dba9ckeXRGvSSP61zSxJ2/D6rqK5MczGhffE2Shyf5iY4ljlVVPSvJPa212yfxfkJYspDkC5YtP3pYt6W01haG23uSvCajL6Ct6O6qelSSDLf3dK5n4lprdw9fxJ9J8n9mCxwLVbU9o/Dx0tba4WH1ljoWLrQPtuKxkCSttdNJ3pjka5PMV9UVw0Nb5v8Py/bBM4bu6tZa+1SS38lsHwfXJvm2qvpARsOTnpbk1zKm40AIS96W5LHDmQ+fneS5SV7XuaaJqqoHV9VDl+4neXqSd638rJn1uiTXD/evT/LajrV0sRQ8Bt+eGT8WhvEeL0ryntbaryx7aMscCxfbB1vpWKiqHVU1P9yfS/LNGY2Ne2OS7xg2m/Xj4EL74K+X/RipjMZCzexx0Fo72Fp7dGvtqozywG2tte/JmI4DF2tNMpx2/atJtiV5cWvtFzqXNFFV9cUZtX4lyRVJXrYV9kFVvTzJU5NcmeTuJP8xyZEkr0ryhUk+mOQ7W2szO3D9IvvgqRl1P7UkH0jyA8vGRs2cqnpKkj9LciIPjAH5yYzGRG2JY2GFffBd2SLHQlV9VUYDrrdl1EDxqtbazw3fj6/IqBvueJLvHVqEZs4K++C2JDuSVJI7kvzgsgH8M6uqnprkf2mtPWtcx4EQBgDQge5IAIAOhDAAgA6EMACADoQwAIAOhDAAgA6EMGDTqKpPDLdXVdV3X+bX/snzlv/vy/n6AOcTwoDN6Kokawphy652fTHnhLDW2v+wxpoA1kQIAzajm5N8XVXdUVX/fph0+FBVvW2YZPgHktHFFqvqjVX1siTvHNYdGSaqf/fSZPVVdXOSueH1XjqsW2p1q+G131VVJ6rqOcte+01V9eqq+uuqeulwRXGAVbnUL0OAaXRjhitZJ8kQpj7aWvuaqvqcJG+uqj8etn1Skq9srb1/WP5XrbW/H6ZleVtV/X5r7caq+uFh4uLzXZfRVeO/OqOZBd5WVX86PLYnyVckOZXkzRnNO/fnl//PBWaRljBgFjw9yfdX1R0ZTTf0iCSPHR5767IAliT/rqrekeQvk3zBsu0u5ilJXj5MZH13kj9J8jXLXvuuYYLrOzLqJgVYFS1hwCyoJP+2tXb0nJWjud/+4bzlb0ryta21T1bVm5I8aAPvu3zuuLPxnQqsgZYwYDP6eJKHLls+muTfVNX2JKmqL6uqB1/geZ+X5CNDAHtckicve+zM0vPP82dJnjOMO9uR5OuTvPWy/BXAluZXG7AZvTPJ2aFb8SVJfi2jrsC3D4Pj702y/wLP+69JfrCq3pnkZEZdkktemOSdVfX21tr3LFv/miRfm+QdSVqS57fWPjyEOIB1q9Za7xoAALYc3ZEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHQhhAAAdCGEAAB0IYQAAHfz/t7pPFpkxTg8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17f6ad30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Use a three-layer Net to overfit 50 training examples by \n",
    "# tweaking just the learning rate and initialization scale.\n",
    "\n",
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "weight_scale = 1e-2\n",
    "learning_rate = 1e-4\n",
    "model = FullyConnectedNet([100, 100],\n",
    "              weight_scale=weight_scale, dtype=np.float64)\n",
    "solver = Solver(model, small_data,\n",
    "                print_every=10, num_epochs=20, batch_size=25,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate,\n",
    "                }\n",
    "         )\n",
    "solver.train()\n",
    "\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to use a five-layer network with 100 units on each layer to overfit 50 training examples. Again you will have to adjust the learning rate and weight initialization, but you should be able to achieve 100% training accuracy within 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(Iteration 1 / 40) loss: 2.302585\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 0 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 1 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 2 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 3 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 4 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 5 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(Iteration 11 / 40) loss: 2.302227\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 6 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 7 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 8 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 9 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 10 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(Iteration 21 / 40) loss: 2.302519\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 11 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 12 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 13 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 14 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 15 / 20) train acc: 0.160000; val_acc: 0.079000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(Iteration 31 / 40) loss: 2.301299\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 16 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 17 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 18 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 19 / 20) train acc: 0.160000; val_acc: 0.112000\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 3L, 32L, 32L)\n",
      "3072\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(25L, 100L)\n",
      "100\n",
      "(50L, 3L, 32L, 32L)\n",
      "3072\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(50L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 3L, 32L, 32L)\n",
      "3072\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(100L, 100L)\n",
      "100\n",
      "(Epoch 20 / 20) train acc: 0.160000; val_acc: 0.112000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAn4AAAHwCAYAAAA4gmJVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAIABJREFUeJzt3X2YnXV97/v3pyHFKNagpN0SiEixQd1YgtPWHqi11kOwDxKwe4PdCh7toe3pg1RPuoW2W7doQdOqtU/KQVvbUqvVkOKm7pQCra27ICFJSXlIQW0LA1VqiECdjSF8zx/rHrIyzEwmmVlrzVr3+3Vdc2Wt3/30uxeL5DO/pztVhSRJkkbfNw26ApIkSeoPg58kSVJLGPwkSZJawuAnSZLUEgY/SZKkljD4SZIktYTBT9JISLIkySNJVi3kvodQj3cm+f2FPu8M13pFkn+aZfsVSS7uR10kDYfDBl0BSe2U5JGut08FHgX2Nu9/sqquPJjzVdVe4IiF3neYVdVPzGW/JPcCr62qv+ptjSQNmsFP0kBU1RPBq2m1+omq+suZ9k9yWFU91o+6ae787yINF7t6JS1KTZfpx5N8LMnDwGuTfG+SG5PsTnJ/kg8kWdrsf1iSSnJc8/6Pmu2fSfJwkr9L8tyD3bfZ/sok/5jka0l+M8nnkrx+jvdxVpLbmjpfn2R117aLk9yX5KEkdyZ5WVP+kiRbm/IvJ9lwgGv8YpIHmnOd11X+R0ne3rz+1iR/3tRjV5LPNuUfA44GPtN0f795DvW+N8n6JDuAf09yUZKPT6nT7yT59bl8RpL6x+AnaTE7C/hj4BnAx4HHgDcBRwGnAmcAPznL8T8O/ArwTOBfgEsOdt8k3wp8AljfXPdLwHfPpfJJng/8IfBzwArgL4GrkyxN8sKm7qdU1bcAr2yuC/CbwIam/ATgk7Nc5hhgGZ3w9lPA7yb5lmn2Ww98sanHfwB+GaCqXgPcB7yyqo6oqvfOVu+u853b1Hl5s+8PT143yTcD5wB/MJfPSVL/GPwkLWZ/W1WfrqrHq2qiqm6uqpuq6rGq+iJwOfD9sxz/yaraUlV7gCuBkw9h3x8BtlfVnzXb3gf82xzrfy5wdVVd3xx7GZ0Q+z10QuxTgBc23aVfau4JYA/wvCTPqqqHq+qmWa7xv4F3VtWeqrqazljJ75hmvz10wuGqqvpGVX32EOs96Teq6t7mv8u9wN8Br262/RAwXlV/P8s1JA2AwU/SYnZP95skJya5Jsm/JnkIeAedVriZ/GvX668z+4SOmfY9urseVVXAvXOo++Sx/9x17OPNsSuraifwFjr38JWmS/s/NLv+X8ALgJ1JPp/kh2a5xr81k1Wmq3u3y5q6XJfkC0nWH0q9u/a5Z8oxHwVe27x+LZ1WQEmLjMFP0mJWU95/CPgH4ISmG/S/AelxHe6n050KQJKwfwCazX3Ac7qO/abmXOMAVfVHVXUq8FxgCXBpU76zqs4FvhX4deBTSZ4yn5uoqoeq6heq6jhgHfBfk0y2lk79nGet9wzHbARe3HRhv5JOq6mkRcbgJ2mYPB34Gp0JBc9n9vF9C+V/AKck+dEkh9EZY7hijsd+AnhVkpc14+PWAw8DNyV5fpIfSHI4MNH8PA6Q5HVJjmpa2r5GJ2Q9Pp+baOr/7U1w/RqdpXMmz/ll4Pi51Hum81fV14GrgI8Bn6uq++ZTX0m9YfCTNEzeApxPJ4R8iM6Ej56qqi/TmajwXuCrwLcD2+iMpTvQsbfRqe/vAg/QmYzyqmbc3OHAe+iMF/xX4Ejgl5pDfwi4o5nN/GvAOVX1jXneymrgeuAR4HN0xuj9TbPtV4H/3szgvfAA9Z7NR4GTsJtXWrTSGa4iSZqLJEvodIX+WFdwEpDkeOBW4Nuq6t8HXR9JT2aLnyQdQJIzkixvumV/hc4M2c8PuFqLSjMO8M3AHxv6pMXLJ3dI0oGdRmc9wcOA24CzquqAXb1tkeQZdCZ+/BOwdrC1kTQbu3olSZJawq5eSZKkljD4SZIktYRj/KZx1FFH1XHHHTfoakiSJB3QLbfc8m9VNaf1RQ1+0zjuuOPYsmXLoKshSZJ0QEn++cB7ddjVK0mS1BIGP0mSpJYw+EmSJLWEwU+SJKklDH6SJEktYfCTJElqCYOfJElSSxj8JEmSWsLgJ0mS1BIGP0mSpJYw+EmSJLWEwU+SJKklDH6SJEktYfCTJElqCYOfJElSSxw26Aq00aZt42zYvJP7dk9w9PJlrF+7mnVrVg66WpIkacQZ/Pps07ZxLtq4g4k9ewEY3z3BRRt3ABj+JElST9nV22cbNu98IvRNmtizlw2bdw6oRpIkqS0Mfn123+6JgyqXJElaKAa/Pjt6+bKDKpckSVooBr8+W792NcuWLtmvbNnSJaxfu3rW4zZtG+fUy67nuW+9hlMvu55N28Z7WU1JkjSCnNzRZ5MTOA5mVq8TQiRJ0kIw+A3AujUrDyqwzTYhxOAnSZLmyq7eIeCEEEmStBBs8RsCRy9fxvg0IW+YJ4S4iLUkSf1ni98QONQJIYvV5JjF8d0TFPvGLDphRZKk3jL4DYF1a1Zy6dknsXL5MgKsXL6MS88+aWhbyFzEWpKkwbCrd0gc7ISQxcwxi5IkDYbBT303imMWpUPleFdJ/WRXr/pu1MYsSofK8a6S+s0WP/XdoSxirUNni9Li5RqdHX5Hpf7pWfBLcizwB8C3AQVcXlW/MWWfM4FLgMeBx4ALq+pvm23nA7/c7PrOqvpokqcCfwp8O7AX+HRVvbXZ//XABmDyV+XfqqorZjrXwt+xDka/xiy2/R8Un/qyuDne1e+o1G+97Op9DHhLVb0AeAnwM0leMGWf64DvrKqTgTcAk0HtmcDbgO8Bvht4W5Ijm2N+rapOBNYApyZ5Zdf5Pl5VJzc/czmXRpjdaM6gXuxmGtfapvGufkel/upZ8Kuq+6tqa/P6YeAOYOWUfR6pqmrePo1OyyDAWuDaqtpVVQ8C1wJnVNXXq+qG5thvAFuBYw5QlWnPNf871GLnPyi2KC12jnf1Oyr1W18mdyQ5jk4L3U3TbDsryZ3ANXRa/aATEO/p2u1epoTGJMuBH6XTajjp1UluTfLJpqt5TufSaPIfFFuUFrtRW6PzUPgdlfqr58EvyRHAp+iM33to6vaquqrpul1HZ7zfXM55GPAx4ANV9cWm+NPAcVX1Ijqtegc1ji/JBUm2JNnywAMPHMyhWqT8B8UWpWGwbs1KPvfWl/Oly36Yz7315a0KfeB3VOq3nga/JEvphL4rq2rjbPtW1WeB45McRWeCxrFdm49h36QNgMuBu6rq/V3Hf7WqHm3eXgG8uHl9oHNNHn95VY1V1diKFSvmdH9a3PwHxRYlLX5+R6X+yr4hdgt84iR0Wt12VdWFM+xzAvCFqqokp9BptTsGOBK4BTil2XUr8OKq2pXkncDzgf9UVY93nevZVXV/8/os4L9W1UuayR3Tnmumuo+NjdWWLVsO+d61eLR9Vq8kafQluaWqxuayby/X8TsVeB2wI8n2puxiYBVAVX0QeDVwXpI9wARwTjPZY1eSS4Cbm+Pe0YS+Y4BfAu4Etnay5RPLtvx8klfRmU28C3h9c51pz9XD+9YiMkqPupMkab561uI3zGzxazdbCSVJw2SxtPhJQ8fFZCVJo8xn9UpdXPtPkjTKDH5SF9f+kySNMrt6tZ+2j287evkyxqcJeW1a+0+SNLps8dMTfLata/9JkkabwU9PcHybi8lKkkabXb16guPbOlz7T5I0qgx+I+xgx+s5vk2SpNFmV++IOpTxeo5vkySNuk3bxjn1sut57luv4dTLrm/VOHYw+I2sQxmv5/g2SdIocxKjXb0j61DH6zm+7dC0fRkcSRoGszWKtOXvbFv8RtRM4/Icr7fw/A1SkoaDkxgNfiNrFMfrLdZxGS6DI0nDwUYRg9/IGrXxeou5Vc3fICVpOIxio8jBcozfCOvXeL1+jG9bzOMyXAZHkobD5L8XbR6TbfDTvEy2xE2GssmWOGBB/0dazK1q69eu3u8zgPb9BilJw6Ltkxjt6tW89Gt822IelzFq3eqSpNFli5/mpV8tcYu9Va3tv0FKkoaDLX6al361xNmqJknS/Nnip3npZ0ucrWqSJM2PwU/z4gwpSZKGh8FP82ZLnCRJw8ExfpIkSS1h8JMkSWoJg58kSVJLOMZP0sD043F/kqR9DH6SBqJfj/uTJO1jV6+kgejX4/4kSfsY/CQNRL8e9ydJ2sfgJ2kg+vW4P0nSPgY/SQOxfu1qli1dsl9Zrx73J0nqcHKHpIHwcX+S1H8GP0kD4+P+JKm/7OqVJElqCYOfJElSSxj8JEmSWsLgJ0mS1BJO7pCkafgcYUmjyOAnSVP4HGFJo8quXkmawucISxpVBj9JmsLnCEsaVQY/SZrC5whLGlUGP0mawucISxpVPQt+SY5NckOS25PcluRN0+xzZpJbk2xPsiXJaV3bzk9yV/NzflP21CTXJLmzOedlXfu/ubnWrUmuS/Kcrm17m2tsT3J1r+5Z0mhYt2Yll559EiuXLyPAyuXLuPTsk5zYIWnopap6c+Lk2cCzq2prkqcDtwDrqur2rn2OAP69qirJi4BPVNWJSZ4JbAHGgGqOfTHwKPA9VXVDkm8GrgN+tao+k+QHgJuq6utJfhp4WVWd01znkao6Yq51Hxsbqy1btizExyBJktRTSW6pqrG57NuzFr+qur+qtjavHwbuAFZO2eeR2pc8n0Yn5AGsBa6tql1V9SBwLXBGVX29qm5ojv0GsBU4pnl/Q1V9vTn+xslySZIkdfRljF+S44A1wE3TbDsryZ3ANcAbmuKVwD1du93LlNCYZDnwo3Ra/aZ6I/CZrvdPabqSb0yy7hBvQ5Ikaaj1fAHnpjv3U8CFVfXQ1O1VdRVwVZKXApcAr5jDOQ8DPgZ8oKq+OGXba+l0EX9/V/Fzqmo8yfHA9Ul2VNUXphx3AXABwKpVqw7mFiVJkoZCT1v8kiylE/qurKqNs+1bVZ8Fjk9yFDAOHNu1+ZimbNLlwF1V9f4p13sF8EvAq6rq0a5zjzd/fhH4Kzqtj1Ovf3lVjVXV2IoVK+Z+k9Ih2rRtnFMvu57nvvUaTr3sejZtGz/wQZIkzUMvZ/UG+DBwR1W9d4Z9Tmj2I8kpwOHAV4HNwOlJjkxyJHB6U0aSdwLPAC6ccq41wIfohL6vdJUfmeTw5vVRwKnA7UgDNPlIsPHdExT7Hglm+JMk9VIvu3pPBV4H7EiyvSm7GFgFUFUfBF4NnJdkDzABnNNM9tiV5BLg5ua4d1TVriTH0GnRuxPY2mTG36qqK4ANwBHAnzbl/1JVrwKeD3woyeN0gu5l3TOLpUGY7ZFgLhkiSeqVngW/qvpbIAfY593Au2fY9hHgI1PK7p3pnFU17djAqvpfwElzqLLUNz4STJI0CD65QxoAHwkmSRoEg580AD4STNJ8OUFMh6Lny7lIerLJcXwbNu/kvt0THL18GevXrnZ8n6Q5mZwgNjlWeHKCGODfI5qVwU8akHVrVvoXtKRD4gQxHSq7eiVJGjJOENOhMvhJkjRknCCmQ2Xwk6Qh46B+OUFMh8oxfpI0RBzUL3CCmA6dwU+ShoiD+jXJCWI6FHb1StIQcVC/pPkw+EnSEHFQv6T5MPhJ0hBxUL+k+XCMnyQNEQf1S5oPg5+kBbFp27hhpE8c1C/pUBn8JM2bS4xI0nBwjJ+keZttiRFJ0uJh8JM0by4xIknDweAnad5cYkSShoPBT9K8ucSIJA0HJ3dImjeXGJGk4WDwk7QgXGJEkhY/g5+kkecag5LUYfCTNNJcY1CS9nFyh6SR5hqDkrSPwU/SSHONQUnax+AnaaS5xqAk7WPwkzTSXGNQkvZxcoc0RJydevBcY1CS9jH4SUPC2amHzjUGJanDrl5pSDg7VZI0XwY/aUg4O1WSNF8GP2lIODtVkjRfBj9pSDg7VZI0X07ukIaEs1MlSfNl8JOGiLNTJUnzYVevJElSSxj8JEmSWsLgJ0mS1BIGP0mSpJYw+EmSJLWEwU+SJKklDH6SJEkt0bPgl+TYJDckuT3JbUneNM0+Zya5Ncn2JFuSnNa17fwkdzU/5zdlT01yTZI7m3Ne1rX/4Uk+nuTuJDclOa5r20VN+c4ka3t1z5IkSYtZLxdwfgx4S1VtTfJ04JYk11bV7V37XAdcXVWV5EXAJ4ATkzwTeBswBlRz7NXAo8CvVdUNSb4ZuC7JK6vqM8AbgQer6oQk5wLvBs5J8gLgXOCFwNHAXyb5jqra28N7lyRJWnR61uJXVfdX1dbm9cPAHcDKKfs8UlXVvH0anZAHsBa4tqp2VdWDwLXAGVX19aq6oTn2G8BW4JjmmDOBjzavPwn8YJI05X9SVY9W1ZeAu4HvXvg7liRJWtz68si2ptt1DXDTNNvOAi4FvhX44aZ4JXBP1273MiU0JlkO/CjwG1OPqarHknwNeFZTfuNs55IkLYxN28Z9nrS0iPU8+CU5AvgUcGFVPTR1e1VdBVyV5KXAJcAr5nDOw4CPAR+oqi8uUD0vAC4AWLVq1UKcUpJaZdO2cS7auIOJPZ2RNOO7J7ho4w6ARRH+DKVSj2f1JllKJ/RdWVUbZ9u3qj4LHJ/kKGAcOLZr8zFN2aTLgbuq6v1dZU8c0wTDZwBfncO5Jq9/eVWNVdXYihUr5niHkqRJGzbvfCL0TZrYs5cNm3cOqEb7TIbS8d0TFPtC6aZtT/rnQBppvZzVG+DDwB1V9d4Z9jmh2Y8kpwCH0wlrm4HTkxyZ5Ejg9KaMJO+kE+ounHK6q4Hzm9c/BlzfjB+8Gji3mfX7XOB5wOcX7k4lSQD37Z44qPJ+WsyhVOqnXnb1ngq8DtiRZHtTdjGwCqCqPgi8GjgvyR5gAjinCWu7klwC3Nwc946q2pXkGOCXgDuBrU1m/K2quoJOyPzDJHcDu+jM5KWqbkvyCeB2OjONf8YZvZK08I5evozxaULe0cuXDaA2+1vMoVTqp54Fv6r6WyAH2OfddJZdmW7bR4CPTCm7d6ZzVtX/Bv7TDNveBbzrwLWWJB2q9WtX7zfGD2DZ0iWsX7t6gLXqWMyhVOonn9whSVoQ69as5NKzT2Ll8mUEWLl8GZeefdKimECxfu1qli1dsl/ZYgmlUj/1ZTkXSVI7rFuzclEEvakm6+SsXrWdwU+SNK1RW/5ksYZSqZ8MfpKkJ1nsa/JJOjSO8ZMkPYnLn0ijyeAnSXoSlz+RRpPBT5L0JDMtc+LyJ9JwM/hJkp7E5U+k0eTkDknSk7j8iTQ/i3VWvMFPkjQtlz+RDs1inhVvV68kSdICWsyz4g1+kiRJC2gxz4o3+EmSJC2gxTwr3uAnSZK0gBbzrHgnd0iSJC2gxTwr3uAn6UkW6zIEkjQsFuuseIOfpP0s5mUIJEnz4xg/SftZzMsQSJLmx+AnaT+LeRkCSdL8GPwk7WcxL0MgSZofg5+k/SzmZQgkSfPj5A5J+1nMyxBIkubH4CfpSRbrMgSSRpfLSPWHwU+SJA2Uy0j1j8FP0lCxVUAaPbMtI+X/3wvL4CdpaNgqII0ml5HqH2f1ShoaLi4tjSaXkeofg5+koWGrgDSaXEaqfwx+koaGrQLSaFq3ZiWXnn0SK5cvI8DK5cu49OyTHMLRA47xkzQ01q9dvd8YP7BVYK6cFKPFzmWk+sPgJ2louLj0oXFSjKRJBj9JQ8VWgYPnUhmSJhn8JGnEjeKkGLuupUPj5A5JGnGjNilmsut6fPcExb6u603bxgddNWnRM/hJ0ogbtaUyXM9ROnR29UrSiBu1STGj2HUt9YvBT5JaYJQmxRy9fBnj04S8Ye26lvrJrl5J0lAZta5rqZ9s8ZMkDZVR67qW+sngJ0kaOqPUdS31k129kiRJLXFQwS8dT+tVZSRJktQ7Bwx+Sf4gybckeSpwG/ClJG/ufdUkSZK0kObS4veiqnoIWAf8BXAM8PoDHZTk2CQ3JLk9yW1J3jTNPmcmuTXJ9iRbkpzWte38JHc1P+d3lb8ryT1JHplyrvc159me5B+T7O7atrdr29VzuGdJkqSRM5fJHUuTHAacCfxuVX0jyeNzOO4x4C1VtTXJ04FbklxbVbd37XMdcHVVVZIXAZ8ATkzyTOBtwBhQzbFXV9WDwKeB3wLu6r5YVf3C5OskPwes6do8UVUnz6HOkiRJI2suLX5XAP8CHAn8dZJVwCOzHwJVdX9VbW1ePwzcAaycss8jVVXN26fRCXkAa4Frq2pXE/auBc5ojrmxqu4/wOVfA3xsDvcmSZLUGgcMflX1vqo6uqpOb0LaPcDLD+YiSY6j0wJ30zTbzkpyJ3AN8IameGVznUn3MiU0znKt5wDPBa7vKn5K05V8Y5J1B1N3SZKkUTGXyR0/m+RbmtcfohPevm+uF0hyBPAp4MJmrOB+quqqqjqRzhjCS+Z63lmcC3yyqrqf4P2cqhoDfhx4f5Jvn6aeFzThcMsDDzywANWQJElaXObS1XtBVT2U5HQ6rW4/DbxnLidPspRO6LuyqjbOtm9VfRY4PslRwDhwbNfmY5qyuTiXKd28VTXe/PlF4K/Yf/zf5D6XV9VYVY2tWLFijpeSJEkaHnMJfpPj7n4I+L2qumUuxyUJ8GHgjqp67wz7nNDsR5JTgMOBrwKbgdOTHJnkSOD0puxA1zyRzljEv+sqOzLJ4c3ro4BTgdunP4MkSdLomsus3r9P8ufAdwAXN123dYBjoBOwXgfsSLK9KbsYWAVQVR8EXg2cl2QPMAGc04wj3JXkEuDm5rh3VNUugCTvodNl+9Qk9wJXVNXbm/3OBf6ka8IIwPOBDzUzkb8JuGzKzGJJkqRWyP4ZaZodkiXAi4G7q2pX02p2bFVt60cFB2FsbKy2bNky6GpIkiQdUJJbmrkMB3TAFr+q2tuEvbObXtm/rqrPzLOOkiRJ6rO5jNV7F/CLwBebn/VJ3tnrikmSJGlhzWWM348Cp1TVYwBJPgJsBX65lxWTJEnSwprLrF6Ap8/wWpIkSUNiLi1+7wG2JrkOCPAy4Fd6WSlJkiQtvLlM7vijJDcA39MU/bfJBZElSfts2jbOhs07uW/3BEcvX8b6tatZt2ZOT5uUpL6YMfgledGUorubP5+V5FlVdWvvqiVJw2XTtnEu2riDiT2dp0WO757goo07AAx/0pAbpV/qZmvx++1ZthXw0gWuiyQNrQ2bdz4R+iZN7NnLhs07h/YfCEmj90vdjMGvqr6vnxWRpGF23+6JgyqXhsUotXYdilH7pW6us3olSbM4evmygyqXhsFka9f47gmKfa1dm7a1Z6j/qP1SZ/CTpAWwfu1qli1dsl/ZsqVLWL929YBqJM3fbK1dbTFqv9QZ/CRpAaxbs5JLzz6JlcuXEWDl8mVcevZJQ9kVJE0atdauQzFqv9QdcDmXaWb3AnwNuKeqHl/4KknScFq3ZqVBTyPl6OXLGJ8m5A1ra9ehmPx/elTGOc5lAecPAycDt9FZwPn5wD8Az0hyQVVd18P6SZKkAVm/dvV+M1phuFu7DtUo/VI3l+B3F/DGyXX7kpwEXAhcCnySTiiUJGnktH1G66i1dmluwe8F3Ys1V9WOJKdU1d1Jelg1SZIGZ9TWbztUo9TapblN7rg7yW8mObX5+QDwhSSHA4/1uH6SJA2EM1o1iuYS/M4D7gXe2vzcB5xPJ/T9YO+qJknS4DijVaPogF29VfV14N3Nz1RfW/AaSZK0CDijVaPogC1+SV6S5DNJbk/yj5M//aicJEmDMmrrt0kwt8kdvwf8InALsPcA+0qSNBKc0apRNJfg91BVfbrnNZEkaZFxRqtGzVyC3/VJLgU2Ao9OFnYv8SJJkqTFby7B77QpfwIU8NKFr44kSZJ6ZS6zer+vHxWRJElSb80Y/JK8pqo+luTnp9teVR/oXbUkSZK00GZr8Tuy+XNFPyoiSZKk3pox+FXV7zR//kr/qiNJkqReOeAYvyRHAW8Ajuvev6ou6F21JEmStNDmMqv3z4Abgb/FBZwlSZKG1lyC39Oq6i09r4kkSZJ66oDP6gU+k+T0ntdEkiRJPTWX4PdTwP9M8kiSXUkeTLKr1xWTJEnSwppLV+9RPa+FJEmSem62BZyfV1V3AS+cYRef1StJkjREZmvxeyvwRuC3p9nms3olSZKGzGwLOL+x+dNn9UqSJI2AuYzxI8mJwAuAp0yWVdUf96pSkiRJWnhzeXLHLwOnAycCm4G1dBZzNvhJkiQNkbks53IO8APA/VX1OuA7mWNLoSRJkhaPuQS/iaraCzyW5OnAvwLH97ZakiRJWmhzabnblmQ58BFgC/AQsLWntZIkSdKCm7XFL0mAt1fV7qr6beCHgZ+sqvMOdOIkxya5IcntSW5L8qZp9jkzya1JtifZkuS0rm3nJ7mr+Tm/q/xdSe5J8siUc70+yQPNubYn+YkDnUuSJKlNUlWz75DcUlUvPugTJ88Gnl1VW5su4luAdVV1e9c+RwD/XlWV5EXAJ6rqxCTPpNO6OEZnzcBbgBdX1YNJXgL8M3BXVR3Rda7XA2NV9bNT6jHjuWaq+9jYWG3ZsuVgb1mSJKnvmqw2Npd95zLG7/NJ1hxsJarq/qra2rx+GLgDWDlln0dqX/J8Gp1gBp2Zw9dW1a4moF0LnNEcc2NV3X8QVZnxXJIkSW0yY/BLMjn+7zTg5iQ7k2xNsi3JQY3xS3IcsAa4aZptZyW5E7gGeENTvBK4p2u3e5kSGmfw6qbr+JNJjp3nuSRJkkbKbJM7Pg+cAqybzwWa7txPARdW1UNTt1fVVcBVSV4KXAK84hAv9WngY1X1aJKfBD4KvPwg6nkBcAHAqlWrDrEKkiRJi9dsXb0BqKovTPczl5MnWUon9F1ZVRtn27eqPgscn+QoYBw4tmvzMU3ZbMd/taoebd5eAUyOS5zTuarq8qoaq6qxFStWzHYpSZKkoTRbi9+KJG+eaWNVvXe2Ezczgj8M3DHTvklOAL7QTO44BTgc+CqdJ4T8apIjm11PBy46wPWe3TX271V0xhRyKOeSJEkaRbMFvyXAETQtf4fgVOB1wI4k25uyi4FVAFX1QeDVwHlJ9gATwDnNZI9dSS5G6fQbAAAUEklEQVQBbm6Oe0dV7QJI8h7gx4GnJrkXuKKq3g78fJJXAY8Bu4DXN9eZ8VySJEltMuNyLkm2VtUpfa7PouByLpIkaVgs1HIuh9rSJ0mSpEVotuD3g32rhSRJknpuxuDnODhJkqTRMpcnd0iSJGkEGPwkSZJawuAnSZLUEgY/SZKkljD4SZIktcRsT+6QJEkHadO2cTZs3sl9uyc4evky1q9dzbo1KwddLQkw+EmStGA2bRvnoo07mNizF4Dx3RNctHEHgOFPi4JdvZIkLZANm3c+EfomTezZy4bNOwdUI2l/Bj9JkhbIfbsnDqpc6jeDnyRJC+To5csOqlzqN4OfJEkLZP3a1SxbumS/smVLl7B+7eoB1Ujan5M7JElaIJMTOJzVq8XK4CdJ0gJat2alQU+Lll29kiRJLWHwkyRJagmDnyRJUksY/CRJklrC4CdJktQSBj9JkqSWMPhJkiS1hMFPkiSpJQx+kiRJLWHwkyRJagmDnyRJUksY/CRJklrC4CdJktQSBj9JkqSWMPhJkiS1hMFPkiSpJQx+kiRJLWHwkyRJaonDBl0BSZLabtO2cTZs3sl9uyc4evky1q9dzbo1KwddLY0gg58kSQO0ads4F23cwcSevQCM757goo07AAx/WnB29UqSNEAbNu98IvRNmtizlw2bdw6oRhplBj9Jkgbovt0TB1UuzYfBT5KkATp6+bKDKpfmw+AnSdIArV+7mmVLl+xXtmzpEtavXT2gGmmUOblDkqQBmpzA4axe9YPBT5KkAVu3ZqVBT31hV68kSVJL9Cz4JTk2yQ1Jbk9yW5I3TbPPmUluTbI9yZYkp3VtOz/JXc3P+V3l70pyT5JHppzrzc21bk1yXZLndG3b21xje5Kre3XPkiRJi1kvu3ofA95SVVuTPB24Jcm1VXV71z7XAVdXVSV5EfAJ4MQkzwTeBowB1Rx7dVU9CHwa+C3grinX2waMVdXXk/w08B7gnGbbRFWd3KsblSRJGgY9a/Grqvuramvz+mHgDmDllH0eqapq3j6NTsgDWAtcW1W7mrB3LXBGc8yNVXX/NNe7oaq+3ry9EThmoe9JkiRpmPVljF+S44A1wE3TbDsryZ3ANcAbmuKVwD1du93LlNB4AG8EPtP1/ilNV/KNSdYdxHkkSZJGRs9n9SY5AvgUcGFVPTR1e1VdBVyV5KXAJcAr5nm919LpIv7+ruLnVNV4kuOB65PsqKovTDnuAuACgFWrVs2nCpIkSYtST1v8kiylE/qurKqNs+1bVZ8Fjk9yFDAOHNu1+Zim7EDXewXwS8CrqurRrnOPN39+EfgrOq2PU69/eVWNVdXYihUrDnQpSZKkodPLWb0BPgzcUVXvnWGfE5r9SHIKcDjwVWAzcHqSI5McCZzelM12vTXAh+iEvq90lR+Z5PDm9VHAqcDt059FkiRpdPWyq/dU4HXAjiTbm7KLgVUAVfVB4NXAeUn2ABPAOc1kj11JLgFubo57R1XtAkjyHuDHgacmuRe4oqreDmwAjgD+tMmS/1JVrwKeD3woyeN0gu5lU2YWS5LUCpu2jfuEkJbLvkm1mjQ2NlZbtmwZdDUkSVowm7aNc9HGHUzs2ftE2bKlS7j07JMMf0MuyS1VNTaXfX1yhyRJLbBh8879Qh/AxJ69bNi8c0A10iAY/CRJaoH7dk8cVLlGk8FPkqQWOHr5soMq12gy+EmS1ALr165m2dIl+5UtW7qE9WtXD6hGGoSeL+AsSZIGb3ICh7N6283gJ0lSS6xbs9Kg13J29UqSJLWEwU+SJKklDH6SJEktYfCTJElqCSd3SJKkBeUzgRcvg58kSVowU58JPL57gos27gAw/C0CdvVKkqQF4zOBFzeDnyRJWjA+E3hxM/hJkqQF4zOBFzeDnyRJWjA+E3hxc3KHJElaMD4TeHEz+EmSpAXlM4EXL7t6JUmSWsLgJ0mS1BIGP0mSpJYw+EmSJLWEwU+SJKklDH6SJEktYfCTJElqCYOfJElSSxj8JEmSWsLgJ0mS1BIGP0mSpJYw+EmSJLWEwU+SJKklDH6SJEktYfCTJElqCYOfJElSSxj8JEmSWsLgJ0mS1BIGP0mSpJYw+EmSJLWEwU+SJKklDH6SJEktYfCTJElqCYOfJElSS/Qs+CU5NskNSW5PcluSN02zz5lJbk2yPcmWJKd1bTs/yV3Nz/ld5e9Kck+SR6ac6/AkH09yd5KbkhzXte2ipnxnkrW9uWNJkqTFrZctfo8Bb6mqFwAvAX4myQum7HMd8J1VdTLwBuAKgCTPBN4GfA/w3cDbkhzZHPPppmyqNwIPVtUJwPuAdzfnegFwLvBC4Azgd5IsWbC7lCRJGhI9C35VdX9VbW1ePwzcAaycss8jVVXN26cBk6/XAtdW1a6qehC4lk5oo6purKr7p7nkmcBHm9efBH4wSZryP6mqR6vqS8DdTB8cJUmSRlpfxvg13a5rgJum2XZWkjuBa+i0+kEnIN7Ttdu9TAmN03jimKp6DPga8KxDPJckSdLI6XnwS3IE8Cngwqp6aOr2qrqqqk4E1gGX9Lo+M0lyQTPOcMsDDzwwqGpIkiT1TE+DX5KldELflVW1cbZ9q+qzwPFJjgLGgWO7Nh/TlM3miWOSHAY8A/jqXM9VVZdX1VhVja1YseIAl5IkSRo+vZzVG+DDwB1V9d4Z9jmh2Y8kpwCH0wlrm4HTkxzZTOo4vSmbzdXA5OzfHwOub8YPXg2c28z6fS7wPODz87s7SZKk4XNYD899KvA6YEeS7U3ZxcAqgKr6IPBq4Lwke4AJ4JwmrO1Kcglwc3PcO6pqF0CS9wA/Djw1yb3AFVX1djoh8w+T3A3sojOTl6q6LckngNvpzDT+mara28P7liRJWpSyb1KtJo2NjdWWLVsGXQ1JkqQDSnJLVY3NZV+f3CFJktQSBj9JkqSWMPhJkiS1hMFPkiSpJQx+kiRJLWHwkyRJagmDnyRJUksY/CRJklrC4CdJktQSBj9JkqSWMPhJkiS1hMFPkiSpJQx+kiRJLWHwkyRJagmDnyRJUksY/CRJklrC4CdJktQSBj9JkqSWMPhJkiS1hMFPkiSpJQx+kiRJLWHwkyRJagmDnyRJUksY/CRJklrC4CdJktQSBj9JkqSWMPhJkiS1hMFPkiSpJQx+kiRJLWHwkyRJagmDnyRJUksY/CRJklrC4CdJktQSBj9JkqSWMPhJkiS1hMFPkiSpJQx+kiRJLWHwkyRJagmDnyRJUksY/CRJklrC4CdJktQSBj9JkqSWMPhJkiS1RM+CX5Jjk9yQ5PYktyV50zT7nJnk1iTbk2xJclrXtvOT3NX8nN9V/uIkO5LcneQDSdKUf7w5z/Yk/5Rke1N+XJKJrm0f7NU9S5IkLWaH9fDcjwFvqaqtSZ4O3JLk2qq6vWuf64Crq6qSvAj4BHBikmcCbwPGgGqOvbqqHgR+F/i/gZuAPwfOAD5TVedMnjTJrwNf67rOF6rq5N7dqiRJ0uLXsxa/qrq/qrY2rx8G7gBWTtnnkaqq5u3T6IQ8gLXAtVW1qwl71wJnJHk28C1VdWNz3B8A67rP2bQA/mfgYz26NUmSpKHUlzF+SY4D1tBppZu67awkdwLXAG9oilcC93Ttdm9TtrJ5PbW82/cBX66qu7rKnptkW5K/TvJ987gVSZKkodXz4JfkCOBTwIVV9dDU7VV1VVWdSKfl7pIFuORr2L+1735gVVWtAd4M/HGSb5mmnhc04wy3PPDAAwtQDUmSpMWlp8EvyVI6oe/Kqto4275V9Vng+CRHAePAsV2bj2nKxpvXU8snr3cYcDbw8a7zPlpVX21e3wJ8AfiOaa5/eVWNVdXYihUrDuo+JUmShkEvZ/UG+DBwR1W9d4Z9TuialXsKcDjwVWAzcHqSI5McCZwObK6q+4GHkrykOe484M+6TvkK4M6qurfrGiuSLGleHw88D/jiAt+uJEnSotfLWb2nAq8DdkwurQJcDKwCqKoPAq8GzkuyB5gAzmkmbexKcglwc3PcO6pqV/P6/wF+H1gGfKb5mXQuT57U8VLgHc01Hgd+qutckiRJrZF9k2o1aWxsrLZs2TLoakiSJB1Qkluqamwu+/rkDkmSpJYw+EmSJLWEwU+SJKklDH6SJEktYfCTJElqCYOfJElSSxj8JEmSWsLgJ0mS1BIGP0mSpJbo5SPbJEmSembTtnE2bN7JfbsnOHr5MtavXc26NSsHXa1FzeAnSZKGzqZt41y0cQcTe/YCML57gos27gAw/M3Crl5JkjR0Nmze+UTomzSxZy8bNu8cUI2Gg8FPkiQNnft2TxxUuToMfpIkaegcvXzZQZWrw+AnSZKGzvq1q1m2dMl+ZcuWLmH92tUDqtFwcHKHJEkaOpMTOJzVe3AMfpIkaSitW7PSoHeQ7OqVJElqCYOfJElSSxj8JEmSWsLgJ0mS1BIGP0mSpJYw+EmSJLWEwU+SJKklDH6SJEktYfCTJElqCYOfJElSSxj8JEmSWsLgJ0mS1BIGP0mSpJYw+EmSJLWEwU+SJKklUlWDrsOik+QB4J/7cKmjgH/rw3UWMz8DPwPwMwA/A/AzAD8D8DOAg/8MnlNVK+ayo8FvgJJsqaqxQddjkPwM/AzAzwD8DMDPAPwMwM8AevsZ2NUrSZLUEgY/SZKkljD4Ddblg67AIuBn4GcAfgbgZwB+BuBnAH4G0MPPwDF+kiRJLWGLnyRJUksY/AYgyRlJdia5O8lbB12fQUjyT0l2JNmeZMug69MvST6S5CtJ/qGr7JlJrk1yV/PnkYOsY6/N8Bm8Pcl4833YnuSHBlnHXkpybJIbktye5LYkb2rKW/M9mOUzaM33ACDJU5J8PsnfN5/Df2/Kn5vkpubfiI8n+eZB17UXZrn/30/ypa7vwcmDrmuvJVmSZFuS/9G879l3wODXZ0mWAL8NvBJ4AfCaJC8YbK0G5geq6uSWTdv/feCMKWVvBa6rqucB1zXvR9nv8+TPAOB9zffh5Kr68z7XqZ8eA95SVS8AXgL8TPN3QJu+BzN9BtCe7wHAo8DLq+o7gZOBM5K8BHg3nc/hBOBB4I0DrGMvzXT/AOu7vgfbB1fFvnkTcEfX+559Bwx+/ffdwN1V9cWq+gbwJ8CZA66T+qSqPgvsmlJ8JvDR5vVHgXV9rVSfzfAZtEZV3V9VW5vXD9P5y34lLfoezPIZtEp1PNK8Xdr8FPBy4JNN+ch+F2a5/1ZJcgzww8AVzfvQw++Awa//VgL3dL2/lxb+hUfnf+6/SHJLkgsGXZkB+7aqur95/a/Atw2yMgP0s0lubbqCR7abs1uS44A1wE209Hsw5TOAln0Pmi6+7cBXgGuBLwC7q+qxZpeR/jdi6v1X1eT34F3N9+B9SQ4fYBX74f3ALwKPN++fRQ+/AwY/DcppVXUKnS7vn0ny0kFXaDGozjT71v3GC/wu8O10unvuB359sNXpvSRHAJ8CLqyqh7q3teV7MM1n0LrvQVXtraqTgWPo9AidOOAq9dXU+0/yH4GL6HwO3wU8E/ivA6xiTyX5EeArVXVLv65p8Ou/ceDYrvfHNGWtUlXjzZ9fAa6i8xdeW305ybMBmj+/MuD69F1Vfbn5B+Bx4P9jxL8PSZbSCTxXVtXGprhV34PpPoO2fQ+6VdVu4Abge4HlSQ5rNrXi34iu+z+jGQpQVfUo8HuM9vfgVOBVSf6JztCvlwO/QQ+/Awa//rsZeF4zY+ebgXOBqwdcp75K8rQkT598DZwO/MPsR420q4Hzm9fnA382wLoMxGTgaZzFCH8fmvE7HwbuqKr3dm1qzfdgps+gTd8DgCQrkixvXi8D/k864x1vAH6s2W1kvwsz3P+dXb8Ahc7YtpH9HlTVRVV1TFUdRycPXF9V/4UefgdcwHkAmiUK3g8sAT5SVe8acJX6KsnxdFr5AA4D/rgtn0GSjwEvA44Cvgy8DdgEfAJYBfwz8J+ramQnP8zwGbyMTvdeAf8E/GTXeLeRkuQ04G+AHewb03MxnTFurfgezPIZvIaWfA8AkryIzsD9JXQaYj5RVe9o/o78EzrdnNuA1zatXyNllvu/HlgBBNgO/FTXJJCRleRlwP9bVT/Sy++AwU+SJKkl7OqVJElqCYOfJElSSxj8JEmSWsLgJ0mS1BIGP0mSpJYw+EnSDJI80vx5XJIfX+BzXzzl/f9ayPNL0nQMfpJ0YMcBBxX8ulbdn8l+wa+q/o+DrJMkHTSDnyQd2GXA9yXZnuQXmgfLb0hyc/Mg+Z+EzgKsSW5I8sfArU3ZpiS3JLktyQVN2WXAsuZ8VzZlk62Lac79D0l2JDmn69x/leSTSe5McmXzZANJmrMD/UYqSYK30qyoD9AEuK9V1XclORz4XJK/aPb9buA/VtWXmvdvqKpdzSOpbk7yqap6a5KfbR5OP9XZdJ5e8Z10nm5yc5LPNtvWAC8E7gM+R+c5n3+78LcraVTZ4idJB+904Lwk2+k8au1ZwPOabZ/vCn0AP5/k74EbgWO79pvJacDHqmpvVX0Z+Gvgu7rOfW9VPU7nUVbHLcjdSGoNW/wk6eAF+Lmq2rxfYedZm/8+5f0rgO+tqq8n+SvgKfO4bvezOvfi3+GSDpItfpJ0YA8DT+96vxn46SRLAZJ8R5KnTXPcM4AHm9B3IvCSrm17Jo+f4m+Ac5pxhCuAlwKfX5C7kNR6/rYoSQd2K7C36bL9feA36HSzbm0mWDwArJvmuP8J/FSSW4GddLp7J10O3Jpka1X9l67yq4DvBf4eKOAXq+pfm+AoSfOSqhp0HSRJktQHdvVKkiS1hMFPkiSpJQx+kiRJLWHwkyRJagmDnyRJUksY/CRJklrC4CdJktQSBj9JkqSW+P8BJMBFFnmajfoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa00ddd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Use a five-layer Net to overfit 50 training examples by \n",
    "# tweaking just the learning rate and initialization scale.\n",
    "\n",
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "learning_rate = 2e-3\n",
    "weight_scale = 1e-5\n",
    "model = FullyConnectedNet([100, 100, 100, 100],\n",
    "                weight_scale=weight_scale, dtype=np.float64)\n",
    "solver = Solver(model, small_data,\n",
    "                print_every=10, num_epochs=20, batch_size=25,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate,\n",
    "                }\n",
    "         )\n",
    "solver.train()\n",
    "\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 2: \n",
    "Did you notice anything about the comparative difficulty of training the three-layer net vs training the five layer net? In particular, based on your experience, which network seemed more sensitive to the initialization scale? Why do you think that is the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "[FILL THIS IN]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update rules\n",
    "So far we have used vanilla stochastic gradient descent (SGD) as our update rule. More sophisticated update rules can make it easier to train deep networks. We will implement a few of the most commonly used update rules and compare them to vanilla SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD+Momentum\n",
    "Stochastic gradient descent with momentum is a widely used update rule that tends to make deep networks converge faster than vanilla stochastic gradient descent. See the Momentum Update section at http://cs231n.github.io/neural-networks-3/#sgd for more information.\n",
    "\n",
    "Open the file `cs231n/optim.py` and read the documentation at the top of the file to make sure you understand the API. Implement the SGD+momentum update rule in the function `sgd_momentum` and run the following to check your implementation. You should see errors less than e-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cs231n.optim import sgd_momentum\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-3, 'velocity': v}\n",
    "next_w, _ = sgd_momentum(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [ 0.1406,      0.20738947,  0.27417895,  0.34096842,  0.40775789],\n",
    "  [ 0.47454737,  0.54133684,  0.60812632,  0.67491579,  0.74170526],\n",
    "  [ 0.80849474,  0.87528421,  0.94207368,  1.00886316,  1.07565263],\n",
    "  [ 1.14244211,  1.20923158,  1.27602105,  1.34281053,  1.4096    ]])\n",
    "expected_velocity = np.asarray([\n",
    "  [ 0.5406,      0.55475789,  0.56891579, 0.58307368,  0.59723158],\n",
    "  [ 0.61138947,  0.62554737,  0.63970526,  0.65386316,  0.66802105],\n",
    "  [ 0.68217895,  0.69633684,  0.71049474,  0.72465263,  0.73881053],\n",
    "  [ 0.75296842,  0.76712632,  0.78128421,  0.79544211,  0.8096    ]])\n",
    "\n",
    "# Should see relative errors around e-8 or less\n",
    "print('next_w error: ', rel_error(next_w, expected_next_w))\n",
    "print('velocity error: ', rel_error(expected_velocity, config['velocity']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have done so, run the following to train a six-layer network with both SGD and SGD+momentum. You should see the SGD+momentum update rule converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_train = 4000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "solvers = {}\n",
    "\n",
    "for update_rule in ['sgd', 'sgd_momentum']:\n",
    "  print('running with ', update_rule)\n",
    "  model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2)\n",
    "\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=5, batch_size=100,\n",
    "                  update_rule=update_rule,\n",
    "                  optim_config={\n",
    "                    'learning_rate': 1e-2,\n",
    "                  },\n",
    "                  verbose=True)\n",
    "  solvers[update_rule] = solver\n",
    "  solver.train()\n",
    "  print()\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "for update_rule, solver in list(solvers.items()):\n",
    "  plt.subplot(3, 1, 1)\n",
    "  plt.plot(solver.loss_history, 'o', label=update_rule)\n",
    "  \n",
    "  plt.subplot(3, 1, 2)\n",
    "  plt.plot(solver.train_acc_history, '-o', label=update_rule)\n",
    "\n",
    "  plt.subplot(3, 1, 3)\n",
    "  plt.plot(solver.val_acc_history, '-o', label=update_rule)\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp and Adam\n",
    "RMSProp [1] and Adam [2] are update rules that set per-parameter learning rates by using a running average of the second moments of gradients.\n",
    "\n",
    "In the file `cs231n/optim.py`, implement the RMSProp update rule in the `rmsprop` function and implement the Adam update rule in the `adam` function, and check your implementations using the tests below.\n",
    "\n",
    "**NOTE:** Please implement the _complete_ Adam update rule (with the bias correction mechanism), not the first simplified version mentioned in the course notes. \n",
    "\n",
    "[1] Tijmen Tieleman and Geoffrey Hinton. \"Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.\" COURSERA: Neural Networks for Machine Learning 4 (2012).\n",
    "\n",
    "[2] Diederik Kingma and Jimmy Ba, \"Adam: A Method for Stochastic Optimization\", ICLR 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test RMSProp implementation\n",
    "from cs231n.optim import rmsprop\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "cache = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'cache': cache}\n",
    "next_w, _ = rmsprop(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.39223849, -0.34037513, -0.28849239, -0.23659121, -0.18467247],\n",
    "  [-0.132737,   -0.08078555, -0.02881884,  0.02316247,  0.07515774],\n",
    "  [ 0.12716641,  0.17918792,  0.23122175,  0.28326742,  0.33532447],\n",
    "  [ 0.38739248,  0.43947102,  0.49155973,  0.54365823,  0.59576619]])\n",
    "expected_cache = np.asarray([\n",
    "  [ 0.5976,      0.6126277,   0.6277108,   0.64284931,  0.65804321],\n",
    "  [ 0.67329252,  0.68859723,  0.70395734,  0.71937285,  0.73484377],\n",
    "  [ 0.75037008,  0.7659518,   0.78158892,  0.79728144,  0.81302936],\n",
    "  [ 0.82883269,  0.84469141,  0.86060554,  0.87657507,  0.8926    ]])\n",
    "\n",
    "# You should see relative errors around e-7 or less\n",
    "print('next_w error: ', rel_error(expected_next_w, next_w))\n",
    "print('cache error: ', rel_error(expected_cache, config['cache']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test Adam implementation\n",
    "from cs231n.optim import adam\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "m = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.7, 0.5, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'm': m, 'v': v, 't': 5}\n",
    "next_w, _ = adam(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.40094747, -0.34836187, -0.29577703, -0.24319299, -0.19060977],\n",
    "  [-0.1380274,  -0.08544591, -0.03286534,  0.01971428,  0.0722929],\n",
    "  [ 0.1248705,   0.17744702,  0.23002243,  0.28259667,  0.33516969],\n",
    "  [ 0.38774145,  0.44031188,  0.49288093,  0.54544852,  0.59801459]])\n",
    "expected_v = np.asarray([\n",
    "  [ 0.69966,     0.68908382,  0.67851319,  0.66794809,  0.65738853,],\n",
    "  [ 0.64683452,  0.63628604,  0.6257431,   0.61520571,  0.60467385,],\n",
    "  [ 0.59414753,  0.58362676,  0.57311152,  0.56260183,  0.55209767,],\n",
    "  [ 0.54159906,  0.53110598,  0.52061845,  0.51013645,  0.49966,   ]])\n",
    "expected_m = np.asarray([\n",
    "  [ 0.48,        0.49947368,  0.51894737,  0.53842105,  0.55789474],\n",
    "  [ 0.57736842,  0.59684211,  0.61631579,  0.63578947,  0.65526316],\n",
    "  [ 0.67473684,  0.69421053,  0.71368421,  0.73315789,  0.75263158],\n",
    "  [ 0.77210526,  0.79157895,  0.81105263,  0.83052632,  0.85      ]])\n",
    "\n",
    "# You should see relative errors around e-7 or less\n",
    "print('next_w error: ', rel_error(expected_next_w, next_w))\n",
    "print('v error: ', rel_error(expected_v, config['v']))\n",
    "print('m error: ', rel_error(expected_m, config['m']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have debugged your RMSProp and Adam implementations, run the following to train a pair of deep networks using these new update rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rates = {'rmsprop': 1e-4, 'adam': 1e-3}\n",
    "for update_rule in ['adam', 'rmsprop']:\n",
    "  print('running with ', update_rule)\n",
    "  model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2)\n",
    "\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=5, batch_size=100,\n",
    "                  update_rule=update_rule,\n",
    "                  optim_config={\n",
    "                    'learning_rate': learning_rates[update_rule]\n",
    "                  },\n",
    "                  verbose=True)\n",
    "  solvers[update_rule] = solver\n",
    "  solver.train()\n",
    "  print()\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "for update_rule, solver in list(solvers.items()):\n",
    "  plt.subplot(3, 1, 1)\n",
    "  plt.plot(solver.loss_history, 'o', label=update_rule)\n",
    "  \n",
    "  plt.subplot(3, 1, 2)\n",
    "  plt.plot(solver.train_acc_history, '-o', label=update_rule)\n",
    "\n",
    "  plt.subplot(3, 1, 3)\n",
    "  plt.plot(solver.val_acc_history, '-o', label=update_rule)\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 3:\n",
    "\n",
    "AdaGrad, like Adam, is a per-parameter optimization method that uses the following update rule:\n",
    "\n",
    "```\n",
    "cache += dw**2\n",
    "w += - learning_rate * dw / (np.sqrt(cache) + eps)\n",
    "```\n",
    "\n",
    "John notices that when he was training a network with AdaGrad that the updates became very small, and that his network was learning slowly. Using your knowledge of the AdaGrad update rule, why do you think the updates would become very small? Would Adam have the same issue?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a good model!\n",
    "Train the best fully-connected model that you can on CIFAR-10, storing your best model in the `best_model` variable. We require you to get at least 50% accuracy on the validation set using a fully-connected net.\n",
    "\n",
    "If you are careful it should be possible to get accuracies above 55%, but we don't require it for this part and won't assign extra credit for doing so. Later in the assignment we will ask you to train the best convolutional network that you can on CIFAR-10, and we would prefer that you spend your effort working on convolutional nets rather than fully-connected nets.\n",
    "\n",
    "You might find it useful to complete the `BatchNormalization.ipynb` and `Dropout.ipynb` notebooks before completing this part, since those techniques can help you train powerful models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_model = None\n",
    "################################################################################\n",
    "# TODO: Train the best FullyConnectedNet that you can on CIFAR-10. You might   #\n",
    "# find batch/layer normalization and dropout useful. Store your best model in  #\n",
    "# the best_model variable.                                                     #\n",
    "################################################################################\n",
    "pass\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test your model!\n",
    "Run your best model on the validation and test sets. You should achieve above 50% accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = np.argmax(best_model.loss(data['X_test']), axis=1)\n",
    "y_val_pred = np.argmax(best_model.loss(data['X_val']), axis=1)\n",
    "print('Validation set accuracy: ', (y_val_pred == data['y_val']).mean())\n",
    "print('Test set accuracy: ', (y_test_pred == data['y_test']).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
